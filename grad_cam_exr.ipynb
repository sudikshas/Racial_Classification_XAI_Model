{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications import resnet_v2\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.python.framework import ops\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "import json\n",
    "import dlib\n",
    "from PIL import Image\n",
    "tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./config/parameters.json\") as f:\n",
    "    data = json.load(f)\n",
    "f.close()\n",
    "\n",
    "model_param = data[\"model_param\"]\n",
    "lr, epochs, batch_size, mapping_path, save_path, log_path = model_param.values()\n",
    "\n",
    "with open(mapping_path) as f:\n",
    "    mapping = json.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0008,\n",
       " 'epochs': 300,\n",
       " 'batch_size': 128,\n",
       " 'mapping_path': './mapping/race.json',\n",
       " 'save_path': './models/race/race_v6.hdf5',\n",
       " 'log_path': './logs/race/race_v6.csv'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Black': 0,\n",
       " 'East Asian': 1,\n",
       " 'Indian': 2,\n",
       " 'Latino_Hispanic': 3,\n",
       " 'Middle Eastern': 4,\n",
       " 'Southeast Asian': 5,\n",
       " 'White': 6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(image_path, im_size = 224, default_max_size=800,size = 300, padding = 0.25):\n",
    "    cnn_face_detector = dlib.cnn_face_detection_model_v1('./dlib_mod/mmod_human_face_detector.dat')\n",
    "    sp = dlib.shape_predictor('./dlib_mod/shape_predictor_5_face_landmarks.dat')\n",
    "    base = 2000  # largest width and height\n",
    "\n",
    "    img = dlib.load_rgb_image(image_path)\n",
    "    old_height, old_width, _ = img.shape\n",
    "    old_height, old_width, _ = img.shape\n",
    "\n",
    "    if old_width > old_height:\n",
    "        new_width, new_height = default_max_size, int(default_max_size * old_height / old_width)\n",
    "    else:\n",
    "        new_width, new_height =  int(default_max_size * old_width / old_height), default_max_size\n",
    "    img = dlib.resize_image(img, rows=new_height, cols=new_width)\n",
    "    dets = cnn_face_detector(img, 1)\n",
    "    num_faces = len(dets)\n",
    "    if num_faces == 0:\n",
    "        print(\"Sorry, there were no faces found in '{}'\".format(image_path))\n",
    "        return\n",
    "    elif num_faces > 1:\n",
    "        print(\"Multiple face in '{}'. A random face will be returned\".format(image_path))\n",
    "    faces = dlib.full_object_detections()\n",
    "    for detection in dets:\n",
    "        rect = detection.rect\n",
    "        faces.append(sp(img, rect))\n",
    "    image = dlib.get_face_chips(img, faces, size=size, padding = padding)[0]\n",
    "\n",
    "    image = Image.fromarray(image, 'RGB')\n",
    "    image = image.resize((im_size, im_size))\n",
    "\n",
    "    #image = np.array(image) / 255.0\n",
    "    #ori_img = np.array(image)\n",
    "    processed_img = resnet_v2.preprocess_input(np.array(image))\n",
    "    processed_img = processed_img[None,:]\n",
    "    return processed_img\n",
    "\n",
    "def target_category_loss(x, category_index, nb_classes):\n",
    "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
    "\n",
    "def target_category_loss_output_shape(input_shape):\n",
    "    return input_shape\n",
    "\n",
    "def normalize(x):\n",
    "    # utility function to normalize a tensor by its L2 norm\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
    "\n",
    "def load_image(path):\n",
    "    img_path = path\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = resnet_v2.preprocess_input(x)\n",
    "    #x = detect_face(img_path)\n",
    "    return x\n",
    "\n",
    "def register_gradient():\n",
    "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
    "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
    "        def _GuidedBackProp(op, grad):\n",
    "            dtype = op.inputs[0].dtype\n",
    "            return grad * tf.cast(grad > 0., dtype) * \\\n",
    "                tf.cast(op.inputs[0] > 0., dtype)\n",
    "\n",
    "def compile_saliency_function(model, activation_layer='conv2d_7'):\n",
    "    input_img = model.input\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    layer_output = layer_dict[activation_layer].output\n",
    "    max_output = K.max(layer_output, axis=3)\n",
    "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
    "    return K.function([input_img, K.learning_phase()], [saliency])\n",
    "\n",
    "def modify_backprop(model, name):\n",
    "    g = tf.compat.v1.get_default_graph()\n",
    "    with g.gradient_override_map({'Relu': name}):\n",
    "\n",
    "        # get layers that have an activation\n",
    "        layer_dict = [layer for layer in model.layers[1:]\n",
    "                      if hasattr(layer, 'activation')]\n",
    "\n",
    "        # replace relu activation\n",
    "        for layer in layer_dict:\n",
    "            if layer.activation == keras.activations.relu:\n",
    "                layer.activation = tf.nn.relu\n",
    "\n",
    "        # re-instanciate a new model\n",
    "        new_model = load_model(save_path)\n",
    "    return new_model\n",
    "\n",
    "def deprocess_image(x):\n",
    "    '''\n",
    "    Same normalization as in:\n",
    "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
    "    '''\n",
    "    if np.ndim(x) > 3:\n",
    "        x = np.squeeze(x)\n",
    "    # normalize tensor: center on 0., ensure std is 0.1\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + 1e-5)\n",
    "    x *= 0.1\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    #if K.image_dim_ordering() == 'th':\n",
    "    #    x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "def grad_cam(input_model, image, category_index, layer_name):\n",
    "    model = input_model\n",
    "    #model = Sequential()\n",
    "    #model.add(input_model)\n",
    "\n",
    "    nb_classes = 7\n",
    "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
    "    model.add(Lambda(target_layer,\n",
    "                     output_shape = target_category_loss_output_shape))\n",
    "\n",
    "    loss = K.sum(model.layers[-1].output)\n",
    "    #conv_output =  [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
    "    conv_output =  [l for l in model.layers if l.name == layer_name][0].output\n",
    "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
    "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
    "\n",
    "    output, grads_val = gradient_function([image])\n",
    "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
    "\n",
    "    weights = np.mean(grads_val, axis = (0, 1))\n",
    "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
    "\n",
    "    for i, w in enumerate(weights):\n",
    "        cam += w * output[:, :, i]\n",
    "\n",
    "    cam = cv2.resize(cam, (224, 224))\n",
    "    cam = np.maximum(cam, 0)\n",
    "    heatmap = cam / np.max(cam)\n",
    "\n",
    "    #Return to BGR [0..255] from the preprocessed image\n",
    "    image = image[0, :]\n",
    "    image -= np.min(image)\n",
    "    image = np.minimum(image, 255)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "    cam = np.float32(cam / 255) + np.float32(image)\n",
    "    cam = 255 * cam / np.max(cam)\n",
    "    return np.uint8(cam), heatmap, image\n",
    "    #return cam, heatmap, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "model = load_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "\n",
    "If you want to input your own image(i.e. images that are not from the FairFace), please uncomment the last line in load_image function and comment out the rest(except the first line)\n",
    "\n",
    "* After you run the code below, it will generate two images: one for grad-cam and one for guided grad-cam. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_img = '/teams/DSC180A_FA20_A00/a01explainableai/a01capstonegroup03/fairface_pad025/val/7.jpg'\n",
    "#sample_img = \"./IMG_5321.jpg\"\n",
    "preprocessed_input = load_image(sample_img)\n",
    "predictions = model.predict(preprocessed_input)\n",
    "predicted_class = np.argmax(predictions)\n",
    "cam, heatmap, img = grad_cam(model, preprocessed_input, predicted_class, \"conv2d_7\")\n",
    "cv2.imwrite(\"gradcam_7.jpg\", cam)\n",
    "\n",
    "register_gradient()\n",
    "guided_model = modify_backprop(model, 'GuidedBackProp')\n",
    "saliency_fn = compile_saliency_function(guided_model)\n",
    "saliency = saliency_fn([preprocessed_input, 0])\n",
    "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
    "cv2.imwrite(\"guided_gradcam_7.jpg\", deprocess_image(gradcam))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
