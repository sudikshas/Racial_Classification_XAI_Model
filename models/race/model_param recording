race_t1.hdf5
- lr: 0.008
- batchsize: 128
- epoch: 300
- dropout: 0.8, 0.3, 0.3


race_t2.hdf5
- lr: 0.008
- batchsize: 128
- epoch: 300
- dropout: 0.8 (no drop out in later ones)

race_t3.hdf5
- lr: 0.008
- batchsize: 128
- epoch: 300
- dropout: 0.8 (no drop out in later ones)
- Leaky Relu activation function


race_t4.hdf5
- lr: 0.008
- batchsize: 128
- epoch: 300
- dropout: 0.7 (no drop out in later ones)
- Relu activation function
- conv layer: 64 64 128 128 128 128 256 256
- decrease factor = 0.5 instead of 0.7

race_t4.hdf5
- lr: 0.008
- batchsize: 128
- epoch: 300
- dropout: 0.7 (no drop out in later ones)
- Relu activation function
- conv layer: 64 64 128 128 128 128 256 256
- decrease factor = 0.5 instead of 0.7
- Nadam optimizer